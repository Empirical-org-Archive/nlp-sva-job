from sva_classifier import get_feedback as get_sva_feedback
# get_feedback currently returns None if no error, otherwise human-readable feedback

"""
This evaluation program takes a file of annotations and returns metrics about
how well our model performs on detecting errors within those annotations.
"""

def read_test_data(annotated_file):
    """
    Reads test data into a list of dictionaries. Each dictionary in the form:
    {
    "original": "Genetic testing are evil.",
    "sva_error": True,
    "correct": "Genetic testing is evil."
    }

    We will currently store only one correct modification, though there may be
    multiple acceptable correct modifications.
    """
    # TODO: Read in test data from existing m2 file
    return [
        {
            "original": "Hence , the social media sites serves as a platform for the connection .",
            "sva_error": True,
            "correct": "Hence , the social media sites serve as a platform for the connection ."
        },
        {
            "original": "Howard Garner studied the effect of multiple intelligences amongst people and stated that interpersonal intelligence is a form of skills on its own - some has it , some does not .",
            "sva_error": True,
            "correct": "Howard Garner studied the effect of multiple intelligences amongst people and stated that interpersonal intelligence is a form of skills on its own - some have it , some do not ."
        },
        {
            "original": "Sometimes, sentences are correct.",
            "sva_error": False,
            "correct": "Sometimes, sentences are correct."
        }
    ]

def evaluate_example(example, strict=False, verbose=True):
    """
    Takes in one example in the dictionary format described above:
    { "original": ---, "sva_error": ---,  "correct": ---}

    If not strict: returns True if our classifier correctly tags the sentence
    as sva_error or not sva_error. False otherwise.

    TODO: If strict: the correct sentence generated by our model must match the correct
    sentence stored in the training example

    If verbose, will print out logs as we evaluate sentences
    """
    feedback = get_sva_feedback(example["original"])
    sva_error_found = True if feedback else False
    matches = (sva_error_found == example["sva_error"])
    if verbose:
        print("Original: {}".format(example["original"]))
        print("SVA Error: {}".format(example["sva_error"]))
        print("Correct: {}".format(example["correct"]))
        if matches:
            print("MATCH $$$$$$$$$$$$$$$$$$$$$$$$$$$$$")
        else:
            print("MISMATCH !!!!!!!!!!!!!!!!!!!!!!!!!!")
        if feedback:
            print(feedback)
        else:
            print("No SVA error found.")
        print("-----------------------------------------------------")
        print()
    return matches


def evaluate_examples(examples, strict=False, verbose=True):
    """
    Evaluates many examples, returns accuracy metrics on those examples.
    Strict and verbose params are identical to evaluate_example
    """
    num_matched = len([ex for ex in examples if evaluate_example(ex, strict=strict, verbose=verbose)])
    accuracy = num_matched/len(examples)
    if verbose:
        print("Accuracy: {}".format(accuracy))
    return accuracy


if __name__ == '__main__':
    test_data = read_test_data(None)
    evaluate_examples(test_data, strict=False, verbose=True)
